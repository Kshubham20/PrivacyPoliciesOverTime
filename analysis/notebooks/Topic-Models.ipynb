{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.stats.stats import pearsonr \n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import os\n",
    "from pprint import pprint\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "import pickle\n",
    "with open(\"../data/deduped_policy_text_v11no_html_with_links_and_emails.pickle\", \"rb\") as f:\n",
    "    df_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1k = df[df.site_url.isin(set(df[df.alexa_rank <= 1000].site_url.unique()))]\n",
    "df_sample = df_all.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_all\n",
    "df = df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_re = re.compile(\"\\#+\")\n",
    "ngram_size = 1\n",
    "\n",
    "ttt = nltk.tokenize.TextTilingTokenizer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return [\" \".join(words) for words in nltk.ngrams(result,ngram_size)]\n",
    "\n",
    "def load_and_preprocess(rowId):\n",
    "    text = df.loc[rowId].policy_text\n",
    "    #sections = sent_tokenize(text)\n",
    "    #return [preprocess(s) for s in sections]\n",
    "    try:\n",
    "        return [preprocess(para) for para in ttt.tokenize(text)]\n",
    "    except:\n",
    "        return [preprocess(text) ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowIds=list(df.index)\n",
    "\n",
    "start = time.time()\n",
    "processed_docs_structured = list(map(load_and_preprocess,rowIds))\n",
    "print(\"Elapsed: %f \" % (time.time() - start))\n",
    "processed_docs = sum(processed_docs_structured,[])\n",
    "\n",
    "segment_map = []\n",
    "prev_stop = 0\n",
    "for i in range(len(rowIds)):\n",
    "    new_stop = prev_stop + len(processed_docs_structured[i])\n",
    "    segment_map.append((prev_stop,new_stop))\n",
    "    prev_stop = new_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For some segment i\n",
    "def get_original_text(i):\n",
    "    for j,(start,stop) in enumerate(segment_map):\n",
    "        if i >= start and i < stop:\n",
    "            offset = i - start\n",
    "            text = df.loc[rowIds[j]].policy_text\n",
    "            sections = sent_tokenize(text)\n",
    "            return sections[offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens should occur at least 50 times to be interesting\n",
    "dictionary.filter_extremes(no_below=50, no_above=0.5, keep_n=100000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BOW_TOPICS = 500\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=BOW_TOPICS, id2word=dictionary, passes=2, workers=20)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TFIDF_TOPICS = 100\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=TFIDF_TOPICS, id2word=dictionary, passes=2, workers=20)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_original_text(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(corpus_tfidf[38])\n",
    "for index, score in sorted(lda_model_tfidf[corpus_tfidf[38]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[38]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each document, find the topics\n",
    "#Topic weight is the max of the topic weight over all sentences\n",
    "\n",
    "doc_top_hits_tfidf = [{} for i in range(len(rowIds))]\n",
    "doc_top_hits_bow = [{} for i in range(len(rowIds))]\n",
    "for i in range(len(rowIds)):\n",
    "    start,end=segment_map[i]\n",
    "    for j in range(start,end):\n",
    "        for index, score in lda_model_tfidf[corpus_tfidf[j]]: #sorted(lda_model_tfidf[corpus_tfidf[j]], key=lambda tup: -1*tup[1]):\n",
    "            doc_top_hits_tfidf[i][index] = max(doc_top_hits_tfidf[i].get(index,0), score)\n",
    "        for index, score in lda_model[bow_corpus[j]]: #sorted(lda_model[bow_corpus[j]], key=lambda tup: -1*tup[1]):\n",
    "            doc_top_hits_bow[i][index] = max(doc_top_hits_bow[i].get(index,0), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics_for(doc_id):\n",
    "    print(\"BOW:\")\n",
    "    for i,s in sorted(doc_top_hits_bow[doc_id].items(),key=lambda x: -x[1]):\n",
    "        print(\"\\t%d: %f; %s\" % (i,s,lda_model.print_topic(i, 10)))\n",
    "    print(\"TF-IDF:\")\n",
    "    for i,s in sorted(doc_top_hits_tfidf[doc_id].items(),key=lambda x: -x[1]):\n",
    "        print(\"\\t%d: %f; %s\" % (i,s,lda_model_tfidf.print_topic(i, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sums_bow = {}\n",
    "top_sums_tfidf = {}\n",
    "for top_hits in doc_top_hits_bow:\n",
    "    for i,s in top_hits.items():\n",
    "        top_sums_bow[i] = top_sums_bow.get(i,0) + s\n",
    "        \n",
    "for top_hits in doc_top_hits_tfidf:\n",
    "    for i,s in top_hits.items():\n",
    "        top_sums_tfidf[i] = top_sums_tfidf.get(i,0) + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_filtered = set([i for i,s in sorted(top_sums_bow.items(),key=lambda x: -x[1])][:20])\n",
    "tfidf_filtered = set([i for i,s in sorted(top_sums_tfidf.items(),key=lambda x: -x[1])][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics_for_filtered(doc_id):\n",
    "    print(\"BOW:\")\n",
    "    for i,s in sorted(doc_top_hits_bow[doc_id].items(),key=lambda x: -x[1]):\n",
    "        if i in bow_filtered: continue\n",
    "        print(\"\\t%d: %f; %s\" % (i,s,lda_model.print_topic(i, 10)))\n",
    "    print(\"TF-IDF:\")\n",
    "    for i,s in sorted(doc_top_hits_tfidf[doc_id].items(),key=lambda x: -x[1]):\n",
    "        if i in tfidf_filtered: continue\n",
    "        print(\"\\t%d: %f; %s\" % (i,s,lda_model_tfidf.print_topic(i, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_counts = {\n",
    "    ys:len(df[df.year_season == ys]) for ys in df.year_season.unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sums_bow_ys = {}\n",
    "for i in range(len(doc_top_hits_bow)):\n",
    "    top_hits = doc_top_hits_bow[i]\n",
    "    ys = df.loc[rowIds[i]].year_season\n",
    "    if ys not in top_sums_bow_ys: top_sums_bow_ys[ys] = {}\n",
    "    for i,s in top_hits.items():\n",
    "        #if i in bow_filtered: continue\n",
    "        top_sums_bow_ys[ys][i] = top_sums_bow_ys[ys].get(i,0) + s\n",
    "        \n",
    "top_sums_tfidf_ys = {}\n",
    "for i in range(len(doc_top_hits_tfidf)):\n",
    "    top_hits = doc_top_hits_tfidf[i]\n",
    "    ys = df.loc[rowIds[i]].year_season\n",
    "    if ys not in top_sums_tfidf_ys: top_sums_tfidf_ys[ys] = {}\n",
    "    for i,s in top_hits.items():\n",
    "        #if i in tfidf_filtered: continue\n",
    "        top_sums_tfidf_ys[ys][i] = top_sums_tfidf_ys[ys].get(i,0) + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"BOW\")\n",
    "for ys, ts in sorted(top_sums_bow_ys.items(),key=lambda x: x[0]):\n",
    "    print(\"\\t%s:\" % ys)\n",
    "    for i,s in sorted(ts.items(),key=lambda x: -x[1])[:20]:\n",
    "        print(\"\\t\\t%d: %f (%s)\" % (i,s,lda_model.print_topic(i, 4)))\n",
    "print(\"TF-IDF\")\n",
    "for ys, ts in sorted(top_sums_tfidf_ys.items(),key=lambda x: x[0]):\n",
    "    print(\"\\t%s:\" % ys)\n",
    "    for i,s in sorted(ts.items(),key=lambda x: -x[1])[:20]:\n",
    "        print(\"\\t\\t%d: %f (%s)\" % (i,s,lda_model_tfidf.print_topic(i, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicIds = set(itertools.chain(*[list(d.keys()) for d in top_sums_tfidf_ys.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_values = list(sorted(list(top_sums_bow_ys.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_timelines = {\n",
    "    topicId: [top_sums_bow_ys[ys].get(topicId,0) / ys_counts[ys] for ys in ys_values] for topicId in topicIds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top(scores_and_topics):\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    \n",
    "    for score, topicId in scores_and_topics[:10]:\n",
    "        print(\"%d: %f (%s)\" % (topicId,score,lda_model.print_topic(topicId, 4)))\n",
    "    \n",
    "    labels=ys_values\n",
    "    x=list(range(len(ys_values)))\n",
    "    ys=[topics_timelines[topicId] for _,topicId in scores_and_topics[10:]]\n",
    "\n",
    "    #Plot\n",
    "    for y in ys:\n",
    "        sns.lineplot(x=labels,y=y,color=\"grey\")\n",
    "\n",
    "    ys=[(topics_timelines[topicId],topicId) for _,topicId in scores_and_topics[:10]]\n",
    "\n",
    "    #Plot\n",
    "    for y,topicId in ys:\n",
    "        fig = sns.lineplot(x=labels,y=y,label=topicId)\n",
    "        \n",
    "    for item in fig.get_xticklabels():\n",
    "        item.set_rotation(45)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = np.percentile(list(itertools.chain(*topics_timelines.values())),5)\n",
    "\n",
    "def get_dip(freqs):\n",
    "    freqs = freqs[-22:]\n",
    "    maxF = max(freqs)\n",
    "    minF = min(freqs)\n",
    "    if minF <= cutoff:\n",
    "        return 0\n",
    "    return (maxF - minF)\n",
    "\n",
    "topics_biggest_dip = sorted([(get_dip(v),k) for k,v in topics_timelines.items()],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top(topics_biggest_dip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = np.percentile(list(itertools.chain(*topics_timelines.values())),5)\n",
    "\n",
    "def get_max(freqs):\n",
    "    freqs = freqs[-22:]\n",
    "    maxF = max(freqs)\n",
    "    return maxF\n",
    "\n",
    "topics_max = sorted([(get_max(v),k) for k,v in topics_timelines.items()],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top(topics_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop over 22 intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = np.percentile(list(itertools.chain(*topics_timelines.values())),5)\n",
    "\n",
    "def get_dip(freqs):\n",
    "    return freqs[-1] - freqs[-22]\n",
    "\n",
    "topics_biggest_dip = sorted([(get_dip(v),k) for k,v in topics_timelines.items()],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top(topics_biggest_dip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
