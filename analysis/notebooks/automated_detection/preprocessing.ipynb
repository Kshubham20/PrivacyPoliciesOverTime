{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/policyphylog/PrivacyPolicyPlagiarism/.venv/lib64/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: thinc.extra.search.Beam size changed, may indicate binary incompatibility. Expected 112 from C header, got 120 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import heapq\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mistune\n",
    "import mistune.renderers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True,nb_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length *= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "with open(\"../../data/deduped_policy_text_v11no_html_with_links_and_emails.pickle\", \"rb\") as f:\n",
    "    df_all = pd.read_pickle(f)\n",
    "df_sample = df_all.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_sample\n",
    "df = df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_end_punct = set((\".\",\"!\",\"?\",'\"',\"'\"))\n",
    "def strip_incomplete_sentences(text,flag_components=False):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    if len(sentences) == 0:\n",
    "        return \"\"\n",
    "    words = nltk.tokenize.word_tokenize(sentences[-1])\n",
    "    if not words[-1] in valid_end_punct:\n",
    "        if flag_components:\n",
    "            sentences[-1] = \"_cut_\" + sentences[-1] + \"_cut_\"\n",
    "        else:\n",
    "            sentences = sentences[:-1]\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "class StraightTextRenderer(mistune.renderers.BaseRenderer):\n",
    "    \n",
    "    def __init__(self,flag_components):\n",
    "        self.flag_components = flag_components\n",
    "    \n",
    "    def text(self, text):\n",
    "        return text\n",
    "\n",
    "    def link(self, link, text=None, title=None):\n",
    "        if text is None:\n",
    "            return \"link\"\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def image(self, src, alt=\"\", title=None):\n",
    "        return \"\"\n",
    "\n",
    "    def emphasis(self, text):\n",
    "        return text\n",
    "\n",
    "    def strong(self, text):\n",
    "        return text\n",
    "\n",
    "    def codespan(self, text):\n",
    "        if self.flag_components:\n",
    "            return \"\\n_codespan_%s_codespan\\n\" % text\n",
    "        else:\n",
    "            return \"\\n\"\n",
    "\n",
    "    def linebreak(self):\n",
    "        if self.flag_components:\n",
    "            return \"\\n_line break_\\n\"\n",
    "        else:\n",
    "            return \"\\n\"\n",
    "\n",
    "    def inline_html(self, html):\n",
    "        if self.flag_components:\n",
    "            return '\\n_inline-html_%s_inline-html_\\n' % html\n",
    "        else:\n",
    "            #HTML isn't prose\n",
    "            return \"\\n\"\n",
    "\n",
    "    def paragraph(self, text):\n",
    "        if text == '': return text\n",
    "        paragraphs = text.split('\\n')\n",
    "        paragraphs = (strip_incomplete_sentences(para,flag_components=self.flag_components) for para in paragraphs)\n",
    "        text = \"\\n\".join(paragraphs)\n",
    "        if self.flag_components:\n",
    "            return \"\\n_paragraph_\\n\" + text + \"\\n_paragraph_\\n\"\n",
    "        else:\n",
    "            return text + \"\\n\"\n",
    "\n",
    "    def heading(self, text, level):\n",
    "        if self.flag_components:\n",
    "            return '\\n_heading %d_ %s\\n' % (level,text)\n",
    "        else:\n",
    "            #Headings aren't prose\n",
    "            return \"\\n\"\n",
    "\n",
    "    def newline(self):\n",
    "        if self.flag_components:\n",
    "            return '\\n_newline_\\n'\n",
    "        else:\n",
    "            return \"\\n\"\n",
    "\n",
    "    def thematic_break(self):\n",
    "        if self.flag_components:\n",
    "            return '\\n_thematic-break_\\n'\n",
    "        else:\n",
    "            return \"\\n\"\n",
    "\n",
    "    def block_text(self, text):\n",
    "        if self.flag_components:\n",
    "            return '\\n_block-text_%s_block-text_\\n' % text\n",
    "        else:\n",
    "            return \"%s\\n\" % text\n",
    "\n",
    "    def block_code(self, code, info=None):\n",
    "        if self.flag_components:\n",
    "            if not code.strip():\n",
    "                return \"\\n\"\n",
    "            else:\n",
    "                return '\\n_block-code_%s_block-code_\\n' % code\n",
    "        else:\n",
    "            #This stuff usually isn't code, treat it as a paragraph\n",
    "            return self.paragraph(code)\n",
    "\n",
    "    def block_quote(self, text):\n",
    "        if self.flag_components:\n",
    "            return '\\n_block-quote_%s_block-quote_\\n' % text\n",
    "        else:\n",
    "            return \"%s\\n\" % text\n",
    "\n",
    "    def block_html(self, html):\n",
    "        if self.flag_components:\n",
    "            return \"\\n_block-html_%s_block-html\\n\" % html\n",
    "        else:\n",
    "            #HTML isn't prose\n",
    "            return  \"\\n\"\n",
    "\n",
    "    def block_error(self, html):\n",
    "        if self.flag_components:\n",
    "            return \"\\n_block-error_%s_block-error\\n\" % html\n",
    "        else:\n",
    "            #Errors aren't prose\n",
    "            return \"\\n\"\n",
    "\n",
    "    def list(self, text, ordered, level, start=None):\n",
    "        if text == '': return text\n",
    "        paragraphs = text.split('\\n')\n",
    "        paragraphs = [strip_incomplete_sentences(para,flag_components=self.flag_components) for para in paragraphs]\n",
    "        text = \"\\n\".join(paragraphs)\n",
    "        if self.flag_components:\n",
    "            return \"\\n_list %s %d_\\n%s\\n_list_\\n\" % (ordered, level, text)\n",
    "        else:\n",
    "            #Lists are inconsistent in how many sentences they represent\n",
    "            #Rule: If all of the rows are sentences, then we'll keep them\n",
    "            #If any stripped list item is empty, return empty for everything\n",
    "            if any((not list_item.strip() for list_item in paragraphs)):\n",
    "                return \"\\n\"\n",
    "            else:\n",
    "                return text + \"\\n\"\n",
    "\n",
    "    def list_item(self, text, level):\n",
    "        return \"%s\\n\" % text\n",
    "    \n",
    "    def strikethrough(self, text):\n",
    "        return \"\"\n",
    "    \n",
    "    def table(self, text):\n",
    "        if self.flag_components:\n",
    "            return '\\n_table_%s_table_\\n' % (text)\n",
    "        else:\n",
    "            return \"\\n\"\n",
    "    \n",
    "    def table_cell(self, content, align=None, is_head=False):\n",
    "        if self.flag_components:\n",
    "            return '\\n_cell_\\n'\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    def table_head(self, content):\n",
    "        if self.flag_components:\n",
    "            return '\\n_head_\\n'\n",
    "        else:\n",
    "            return \"\"\n",
    "        \n",
    "    def table_row(self, content):\n",
    "        if self.flag_components:\n",
    "            return '_row_%s_row_\\n' % content\n",
    "        else:\n",
    "            return \"\"\n",
    "        \n",
    "    def table_body(self, content):\n",
    "        if self.flag_components:\n",
    "            return '_body_%s_body_\\n' % content\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "markdown = mistune.create_markdown(renderer=StraightTextRenderer(False))\n",
    "markdown_debug = mistune.create_markdown(renderer=StraightTextRenderer(True))\n",
    "\n",
    "#Install mistune plugins\n",
    "import mistune.plugins\n",
    "mistune.plugins.plugin_table(markdown)\n",
    "mistune.plugins.plugin_strikethrough(markdown)\n",
    "mistune.plugins.plugin_table(markdown_debug)\n",
    "mistune.plugins.plugin_strikethrough(markdown_debug)\n",
    "\n",
    "def clean(policy_text):\n",
    "    return markdown(policy_text)\n",
    "\n",
    "def clean_debug(policy_text):\n",
    "    return markdown_debug(policy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365183f2ad2f43ab8e10f9b712cc1033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=910546.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"policy_text_cleaned\"] = df.policy_text.swifter.apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove website-specific terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "truste_regex = re.compile(r\"(?<![a-z])truste(?![a-z])\",flags=re.IGNORECASE)\n",
    "#https://emailregex.com/\n",
    "email_regex = re.compile(r\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\\\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\\\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\",flags=re.IGNORECASE)\n",
    "#https://gist.github.com/gruber/8891611\n",
    "url_regex = re.compile(r\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\",flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9d2da2656e48f181ede5ef75951422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=910546.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66414b8284ab4ab78e1efb9a66a837c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=910546.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee59068bad644cb39ba31d3eee4506f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=910546.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def truste_sub(text):\n",
    "    return truste_regex.sub(\"TrustArc\",text)\n",
    "\n",
    "def email_sub(text):\n",
    "    return email_regex.sub(\"email_sub\",text)\n",
    "\n",
    "def url_sub(text):\n",
    "    return url_regex.sub(\"url_sub\",text)\n",
    "\n",
    "df[\"policy_text_cleaned\"] = df.policy_text_cleaned.swifter.apply(truste_sub).swifter.apply(email_sub).swifter.apply(url_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd824680ad2467e9c308f6163ee7923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=910546.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First pass -- find entities\n",
    "\n",
    "TAGS_TO_SWAP = [\n",
    "    \"ORG\",\n",
    "    \"PERSON\",\n",
    "    #    \"FAC\",\n",
    "    #\"WORK_OF_ART\",\n",
    "]\n",
    "\n",
    "def get_entities(text):\n",
    "    entities = set()\n",
    "    doc = nlp(text)\n",
    "    for entity in sorted(doc.ents, key=lambda x: -len(x.text)):\n",
    "        if entity.label_ in TAGS_TO_SWAP:\n",
    "            entities.add(entity.text.lower())\n",
    "    return entities\n",
    "\n",
    "entities = sorted(list(set(itertools.chain.from_iterable(df.policy_text_cleaned.swifter.apply(get_entities)))),key=len,reverse=True)\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blacklist = [\"email\",\"url\",\"number\"]\n",
    "entities_filtered = list(filter(lambda x: x in blacklist or any(map(str.isalpha,x)),entities))\n",
    "print(len(entities_filtered))\n",
    "print(entities_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_re = re.compile(r\"\\b(?:%s)\\b\" % \"|\".join(map(re.escape,entities_filtered)),re.IGNORECASE)\n",
    "num_regex = re.compile(r\"(?<![a-z])\\d+(\\.\\d+)?(?![a-z])\",flags=re.IGNORECASE)\n",
    "\n",
    "def entity_sub(text):\n",
    "    return entity_re.sub(\"ENTITY\",text)\n",
    "\n",
    "def num_sub(text):\n",
    "    return num_regex.sub(\"NUMBER\",text)\n",
    "\n",
    "df[\"policy_tex_cleaned\"] = df.policy_text_cleaned.swifter.apply(entity_sub).map(num_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text,n=2):\n",
    "    ngrams = set()\n",
    "    for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "        if n is None:\n",
    "            sentence = sys.intern(sentence)\n",
    "            ngrams.add(sentence)\n",
    "            continue\n",
    "        words = nltk.tokenize.word_tokenize(sentence)\n",
    "        words = filter(str.isalnum,words)\n",
    "        phrases = (\" \".join(phrase) for phrase in nltk.ngrams(words,n))\n",
    "        phrases = map(sys.intern,phrases)\n",
    "        ngrams.update(phrases)\n",
    "    return ngrams\n",
    "        \n",
    "counters = {}\n",
    "for ys, sub_df in df.groupby(\"year_season\"):\n",
    "    counters[ys] = collections.Counter(itertools.chain.from_iterable(sub_df[\"policy_text_cleaned\"].swifter.apply(get_ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_values = sorted(list(counters.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = set(itertools.chain.from_iterable(map(collections.Counter.elements,counters.values())))\n",
    "trends = {\n",
    "    phrase: [counters[ys][phrase] for ys in ys_values] for phrase in phrases\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del phrases\n",
    "for ys in ys_values:\n",
    "    del counters[ys]\n",
    "del counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_phrases(key,topN=50):\n",
    "    ranks = (\n",
    "        (key(counts),phrase) for phrase, counts in trends.items()\n",
    "    )\n",
    "    heap = []\n",
    "    for rank in ranks:\n",
    "        if len(heap) < topN:\n",
    "            heapq.heappush(heap,rank)\n",
    "        else:\n",
    "            heapq.heappushpop(heap,rank)\n",
    "    return sorted(heap,reverse=True)\n",
    "\n",
    "rank_phrases(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
